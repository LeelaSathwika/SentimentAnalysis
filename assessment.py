# -*- coding: utf-8 -*-
"""Assessment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zd1rCYJqJ8S52wpBXnweg7_fdbICTCkc

Install Libraries and load the dataset from the nltk
"""

!pip install vaderSentiment transformers textblob

import nltk
nltk.download('twitter_samples')
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

"""Load the nltk-twitter samples data"""

from nltk.corpus import twitter_samples, stopwords, wordnet
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk.util import ngrams
import pandas as pd
import re

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from textblob import TextBlob
from transformers import pipeline

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.metrics import accuracy_score, classification_report

import nltk
nltk.download('punkt_tab')

"""Differentiate the positive and Negative sampples"""

positive_tweets = twitter_samples.strings('positive_tweets.json')
negative_tweets = twitter_samples.strings('negative_tweets.json')

df_pos = pd.DataFrame(positive_tweets, columns=['text'])
df_pos['label'] = 'positive'

df_neg = pd.DataFrame(negative_tweets, columns=['text'])
df_neg['label'] = 'negative'

df = pd.concat([df_pos, df_neg]).reset_index(drop=True)

"""Using Lemmatizer"""

stop_words = set(stopwords.words('english'))
stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()

"""PREPROCESS THE DATA

1.Converting into lower case

2.Regex cleaning

3.Remove Stopwords

4.Tokenization

5.Stemming

6.Lemmatization

7.Generate n-grams
"""

def preprocess_text(text):
    text = text.lower()  # lowercase
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # regex cleaning
    words = [w for w in text.split() if w not in stop_words]  # remove stopwords
    tokens = word_tokenize(' '.join(words))  # tokenize
    stemmed = [stemmer.stem(word) for word in tokens]  # stemming
    lemmatized = [lemmatizer.lemmatize(word) for word in stemmed]  # lemmatization
    bigrams = list(ngrams(lemmatized, 2))
    trigrams = list(ngrams(lemmatized,3))
    fourgrams = list(ngrams(lemmatized,4))
      # generate bigrams
    return {
        'clean_text': ' '.join(lemmatized),
        'tokens': tokens,
        'stemmed': stemmed,
        'lemmatized': lemmatized,
        'bigrams': bigrams,
        'trigrams': trigrams,
        'fourgrams': fourgrams
    }

processed = df['text'].apply(preprocess_text)
df['clean_text'] = processed.apply(lambda x: x['clean_text'])
df['tokens'] = processed.apply(lambda x: x['tokens'])
df['stemmed'] = processed.apply(lambda x: x['stemmed'])
df['lemmatized'] = processed.apply(lambda x: x['lemmatized'])
df['bigrams'] = processed.apply(lambda x: x['bigrams'])
df['trigrams']    = processed.apply(lambda x: x['trigrams'])
df['fourgrams']   = processed.apply(lambda x: x['fourgrams'])

"""Printing the preprocessed data"""

print(df[['text', 'clean_text', 'tokens', 'stemmed', 'lemmatized', 'bigrams','trigrams','fourgrams']].head())

"""Valence Aware Dictionary and sEntiment Reasoner(VADER Model)

Rule-based lexicon and sentiment analysis tool

Purpose: Designed for sentiment analysis of social media text, such as tweets, comments, and short reviews.

Combines all scores into one value between -1 and 1:

0.05 → Positive

< -0.05 → Negative

-0.05 to 0.05 → Neutral
"""

analyzer = SentimentIntensityAnalyzer()
df['vader_score'] = df['clean_text'].apply(lambda x: analyzer.polarity_scores(x)['compound'])
df['vader_sentiment'] = df['vader_score'].apply(lambda score: 'positive' if score > 0 else ('negative' if score < 0 else 'neutral'))

def vader_sentiment(compound):
    if compound >= 0.05:
        return "positive"
    elif compound <= -0.05:
        return "negative"
    else:
        return "neutral"

df['vader_sentiment'] = df['vader_score'].apply(vader_sentiment)

"""Textblob Model:TextBlob is a Python library for processing textual data. It provides a simple API for common natural language processing (NLP) tasks

Key Components:

Polarity (-1 to 1)

Measures sentiment:

-1 = very negative

0 = neutral

1 = very positive
"""

def textblob_sentiment(text):
    polarity = TextBlob(text).sentiment.polarity
    if polarity > 0:
        return 'positive'
    elif polarity < 0:
        return 'negative'
    else:
        return 'neutral'

df['textblob_sentiment'] = df['clean_text'].apply(textblob_sentiment)

"""Load the pretrained model for sentimnet analysis -- distilbert-base-uncased-finetuned-sst-2-english"""

sentiment_model = pipeline("sentiment-analysis", model="distilbert-base-uncased-finetuned-sst-2-english")

def pretrained_sentiment(text):
    result = sentiment_model(text[:512])[0]  # truncate long text
    return result['label'].lower()  # 'POSITIVE' or 'NEGATIVE'

df['pretrained_sentiment'] = df['clean_text'].apply(pretrained_sentiment)

"""Vader Sentimnet Model Distribution"""

vader_counts = df['vader_sentiment'].value_counts()
plt.figure(figsize=(6,4))
sns.barplot(x=vader_counts.index, y=vader_counts.values, palette='pastel')
plt.title('VADER Sentiment Distribution')
plt.show()

"""Textblob Model Sentiment Distribution"""

textblob_counts = df['textblob_sentiment'].value_counts()
plt.figure(figsize=(6,4))
sns.barplot(x=textblob_counts.index, y=textblob_counts.values, palette='pastel')
plt.title('TextBlob Sentiment Distribution')
plt.show()

# VADER Accuracy
vader_acc = accuracy_score(df['label'], df['vader_sentiment'])
print("VADER Accuracy:", vader_acc)

# TextBlob Accuracy
textblob_acc = accuracy_score(df['label'], df['textblob_sentiment'])
print("TextBlob Accuracy:", textblob_acc)

"""VADER Model and TextBlob Accuracy"""

print("\nVADER Classification Report:")
print(classification_report(df['label'], df['vader_sentiment']))

print("\nTextBlob Classification Report:")
print(classification_report(df['label'], df['textblob_sentiment']))

"""Pretrained Model Sentimnet Distribution"""

pretrained_counts = df['pretrained_sentiment'].value_counts()
plt.figure(figsize=(6,4))
sns.barplot(x=pretrained_counts.index, y=pretrained_counts.values, palette='pastel')
plt.title('Pretrained Model Sentiment Distribution')
plt.show()

"""Compare both the Rule based models and the Pretrained Model"""

import matplotlib.pyplot as plt
import seaborn as sns

# Count predictions for each model
vader_counts = df['vader_sentiment'].value_counts()
textblob_counts = df['textblob_sentiment'].value_counts()
pretrained_counts = df['pretrained_sentiment'].value_counts()

# Combine into one DataFrame
comparison_df = pd.DataFrame({
    'VADER': vader_counts,
    'TextBlob': textblob_counts,
    'Pretrained': pretrained_counts
}).fillna(0)  # fill missing values with 0

# Plot
comparison_df.T.plot(kind='bar', figsize=(8,5), color=['#ff9999','#99ff99','#9999ff'])
plt.title('Sentiment Predictions Comparison')
plt.ylabel('Count')
plt.xticks(rotation=0)
plt.legend(title='Sentiment')
plt.show()

import nltk
from textblob import TextBlob
from nltk.sentiment.util import mark_negation, extract_unigram_feats

# Download resources
nltk.download('twitter_samples')
nltk.download('punkt')

from nltk.corpus import twitter_samples

# Load dataset
positive_tweets = twitter_samples.tokenized('positive_tweets.json')
negative_tweets = twitter_samples.tokenized('negative_tweets.json')

# Negation marking
all_positive = [mark_negation(doc) for doc in positive_tweets]
all_negative = [mark_negation(doc) for doc in negative_tweets]

# Extract unigrams
all_words = [word for tweet in (all_positive + all_negative) for word in tweet]
freq_dist = nltk.FreqDist(all_words)
unigram_features = list(freq_dist.keys())[:200]

# Build feature sets
positive_features = [(extract_unigram_feats(doc, unigram_features), 'Positive') for doc in all_positive]
negative_features = [(extract_unigram_feats(doc, unigram_features), 'Negative') for doc in all_negative]

# Combine and train
train_set = positive_features + negative_features
classifier = nltk.NaiveBayesClassifier.train(train_set)

print("Training complete ✅")

#  Prediction Function
def predict_sentiment(user_input):
    # TextBlob sentiment
    blob = TextBlob(user_input)
    tb_polarity = blob.sentiment.polarity
    tb_sentiment = "Positive" if tb_polarity > 0 else "Negative" if tb_polarity < 0 else "Neutral"

    # NLTK sentiment
    tokens = nltk.word_tokenize(user_input)
    features = extract_unigram_feats(tokens, unigram_features)
    nltk_sentiment = classifier.classify(features)

    print(f"\nUser Input: {user_input}")
    print(f"TextBlob Sentiment: {tb_sentiment} (polarity={tb_polarity})")
    print(f"NLTK Sentiment: {nltk_sentiment}")

# Example
user_input = input("Enter a statement: ")
predict_sentiment(user_input)

"""Conclusion:

DistilBERT: Most reliable, context-aware, high accuracy. Ideal for production.

VADER: Quick, interpretable, best for informal text and social media.

TextBlob: Simple, lightweight, good for prototyping, but lower accuracy.

Practical Comparison of Sentiment Analysis Models:

VADER:

Accuracy: ~70–75%

Performs well on short, informal text, like tweets or product reviews.

Handles punctuation, capitalization, and negations.

Struggles with long, complex sentences or domain-specific vocabulary.

TextBlob:

Accuracy: ~60–65%

Lexicon-based, best for formal text.

Often biased toward neutral or positive predictions.

Poor handling of slang, and punctuation emphasis.

DistilBERT (distilbert-base-uncased-finetuned-sst-2-english):

Accuracy: ~85–90%

Deep learning model, context-aware, handles negation, sarcasm, and long sentences.

Predicts only binary sentiment (positive/negative).

Slightly slower than lexicon-based models but highly accurate.
"""
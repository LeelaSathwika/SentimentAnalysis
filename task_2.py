# -*- coding: utf-8 -*-
"""Task 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y2X4U7h0CdA9Ex9j9x0K20dUhh7qaAtu
"""

import nltk
nltk.download('movie_reviews')
from nltk.corpus import movie_reviews
import pandas as pd
import re
from nltk.corpus import stopwords
nltk.download('stopwords')

texts = []
labels = []
for fileid in movie_reviews.fileids():
    texts.append(movie_reviews.raw(fileid))
    labels.append(movie_reviews.categories(fileid)[0])

df = pd.DataFrame({'text': texts, 'label': labels})

# Map labels to numeric
df['label'] = df['label'].map({'pos': 1, 'neg': 0})

"""Preprocess text function"""

stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    # Lowercase all text
    text = text.lower()
    # Remove HTML tags (if any)
    text = re.sub(r'<.*?>', '', text)
    # Remove non-alphabetic characters and punctuation
    text = re.sub(r'[^a-z\s]', '', text)
    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    # Remove stopwords
    tokens = text.split()
    tokens = [word for word in tokens if word not in stop_words]
    return " ".join(tokens)

"""Apply preprocessing"""

df['clean_text'] = df['text'].apply(preprocess_text)

print(df[["text", "clean_text", "label"]].head(10))

"""TF-IDF Vectorization and Logistic Regression Classification"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

X_train, X_test, y_train, y_test = train_test_split(
    df['clean_text'], df['label'], test_size=0.2, random_state=42
)

vectorizer = TfidfVectorizer(max_features=5000)
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

X_train_df = pd.DataFrame(X_train_vec.toarray(), columns=vectorizer.get_feature_names_out())

# Display first 5 rows of TF-IDF output
print("\nðŸ”¹ Feature Names (first 20 words):")
print(vectorizer.get_feature_names_out()[:20])

print("\nðŸ”¹ Vectorized Representation of First 5 Reviews:")
print(X_train_df.head())

import numpy as np

def top_tfidf_words(row_sparse, feature_names, top_n=10):
    row = row_sparse.toarray().ravel()
    top_idx = np.argsort(row)[-top_n:][::-1]        # indices of top scores
    return [(feature_names[i], row[i]) for i in top_idx if row[i] > 0]

feature_names = vectorizer.get_feature_names_out()
print(top_tfidf_words(X_train_vec[0], feature_names, top_n=10))

"""Display TF-IDF vectorization output for the first few documents"""

import pandas as pd
tfidf_array = X_train_vec.toarray()
feature_names = vectorizer.get_feature_names_out()
tfidf_df = pd.DataFrame(tfidf_array, columns=feature_names)
print("TF-IDF features for first document:")
print(tfidf_df.iloc[0].sort_values(ascending=False).head(10))  # top 10 terms for first doc

"""Train logistic regression"""

r = LogisticRegression(max_iter=200)
r.fit(X_train_vec, y_train)

y_pred = r.predict(X_test_vec)
print("\nLogistic Regression Classification Report:\n", classification_report(y_test, y_pred))

"""Topic Modeling with LDA"""

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

count_vect = CountVectorizer(max_features=4000, stop_words='english')
X_counts = count_vect.fit_transform(df['clean_text'])

lda = LatentDirichletAllocation(n_components=5, random_state=42)
lda.fit(X_counts)

def print_topics(model, vectorizer, n_top_words=10):
    words = vectorizer.get_feature_names_out()
    for idx, topic in enumerate(model.components_):
        print(f"Topic {idx}: ", " | ".join([words[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))

print("\nLDA Topics:")
print_topics(lda, count_vect)

"""Transformer-based Sentiment Classification (BERT)

Split data â†’ train/test.

Convert to Hugging Face Dataset.

Tokenize text for BERT.

Set PyTorch tensor format.

Load pre-trained BERT model.

Define training arguments.

Initialize Trainer.

Fine-tune the model (trainer.train()).

Evaluate and save the model.
"""

from datasets import Dataset
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split # Import train_test_split

train_texts, test_texts, train_labels, test_labels = train_test_split(
    df['text'].tolist(),  # Use original unprocessed text for BERT tokenization
    df['label'].tolist(), test_size=0.2, random_state=42
)

train_dataset = Dataset.from_dict({'text': train_texts, 'label': train_labels})
test_dataset = Dataset.from_dict({'text': test_texts, 'label': test_labels})

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

def tokenize_function(examples):
    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)

train_dataset = train_dataset.map(tokenize_function, batched=True)
test_dataset = test_dataset.map(tokenize_function, batched=True)

train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])
test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])

model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

training_args = TrainingArguments(
    output_dir='./results',
    eval_strategy='epoch',  # Correct parameter name
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=2,
    logging_dir='./logs',
    logging_steps=50,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
)

trainer.train()
trainer.save_model('./results') # Save the trained model
results = trainer.evaluate()
print("\nBERT Evaluation Results:", results)

"""c2d06ed5ec50bdb7784539523b25f142b4e68478"""

import torch
import torch.nn.functional as F
from transformers import BertTokenizer, BertForSequenceClassification

# Load your fine-tuned model and tokenizer (change path if saved locally)
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained('./results')  # your fine-tuned model directory

model.eval()  # Set model to evaluation mode

def predict_sentiment(text_list):
    inputs = tokenizer(text_list, padding=True, truncation=True, max_length=128, return_tensors="pt")
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        probs = F.softmax(logits, dim=1)
        preds = torch.argmax(probs, dim=1)

    label_map = {0: "Negative", 1: "Positive"}
    results = []
    for text, pred, prob in zip(text_list, preds, probs):
        results.append({
            "text": text,
            "predicted_label": label_map[pred.item()],
            "confidence": prob[pred].item()
        })
    return results

# Example usage: take text input from user
user_input = input("Enter your text for sentiment analysis:\n")
output = predict_sentiment([user_input])

for res in output:
    print(f"Text: {res['text']}")
    print(f"Sentiment: {res['predicted_label']} (Confidence: {res['confidence']:.4f})")